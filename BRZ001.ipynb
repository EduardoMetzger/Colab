{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiTXEOv6J43Rt6a+spV/ti",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EduardoMetzger/Colab/blob/main/BRZ001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4SKWcaJ2s5hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "import glob\n",
        "import os\n",
        "import pytz\n",
        "from datetime import datetime\n",
        "\n",
        "# Migra os arquivos salvos pelo crawler para a BRZ001 (tabela que contém os dados originais e completos de captura por dia)\n",
        "\n",
        "def create_db_connection(db_file):\n",
        "    \"\"\" Cria uma conexão com o banco de dados SQLite especificado. \"\"\"\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_file)\n",
        "        print(\"Conexão estabelecida com SQLite\")\n",
        "        return conn\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return None\n",
        "\n",
        "def create_table(conn, table_name):\n",
        "    \"\"\" Cria uma tabela no banco de dados especificado pela string SQL. \"\"\"\n",
        "    try:\n",
        "        c = conn.cursor()\n",
        "        c.execute(f'''\n",
        "            CREATE TABLE IF NOT EXISTS {table_name} (\n",
        "                URLPRO TEXT,\n",
        "                IMGPRO TEXT,\n",
        "                PAGPRO TEXT,\n",
        "                NOMPRO TEXT,\n",
        "                VLRDDE TEXT,\n",
        "                VLRPOR TEXT,\n",
        "                QTVEND TEXT,\n",
        "                AVAMED TEXT,\n",
        "                QTAVAL TEXT,\n",
        "                NOMVEN TEXT,\n",
        "                TAMVEN TEXT,\n",
        "                CARPRO TEXT,\n",
        "                DATCAP TEXT,\n",
        "                HORCAP TEXT,\n",
        "                INFORI TEXT,\n",
        "                DATGER TEXT,\n",
        "                HORGER TEXT,\n",
        "                PRIMARY KEY (URLPRO, DATCAP, HORCAP)\n",
        "            );\n",
        "        ''')\n",
        "        conn.commit()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "def process_files_to_db(folder_path, db_file, table_name, output_excel_path, process_date=None):\n",
        "    all_data = pd.DataFrame()  # DataFrame para acumular todos os dados\n",
        "    conn = create_db_connection(db_file)\n",
        "\n",
        "    # Se uma data de processamento não for fornecida, use a data atual\n",
        "    if process_date is None:\n",
        "        process_date = datetime.now().strftime('%Y%m%d')\n",
        "\n",
        "    folder_path = os.path.join(folder_path, process_date)  # Define o caminho para a pasta do dia especificado\n",
        "    pattern = os.path.join(folder_path, '*.xlsx')\n",
        "\n",
        "    # Configuração do fuso horário de São Paulo\n",
        "    timezone_sao_paulo = pytz.timezone('America/Sao_Paulo')\n",
        "\n",
        "    # Carrega as chaves existentes da tabela para evitar inserir duplicatas\n",
        "    existing_keys = pd.read_sql_query(f\"SELECT URLPRO, DATCAP, HORCAP FROM {table_name}\", conn)\n",
        "\n",
        "    for filename in glob.glob(pattern):\n",
        "        try:\n",
        "            df = pd.read_excel(filename, engine='openpyxl')\n",
        "\n",
        "            # Adicionando o caminho de informação\n",
        "            df['INFORI'] = filename\n",
        "\n",
        "            # Renomeando colunas\n",
        "            df.rename(columns={\n",
        "                'Href': 'URLPRO',\n",
        "                'Src': 'IMGPRO',\n",
        "                'Origem': 'PAGPRO',\n",
        "                'Nome do Produto': 'NOMPRO',\n",
        "                'Preço De': 'VLRDDE',\n",
        "                'Preço Por': 'VLRPOR',\n",
        "                'Quantidade Vendida': 'QTVEND',\n",
        "                'Avaliação Final': 'AVAMED',\n",
        "                'Quantidade de Avaliações': 'QTAVAL',\n",
        "                'Vendido Por': 'NOMVEN',\n",
        "                'Tamanho do Vendedor': 'TAMVEN',\n",
        "                'Características do Produto': 'CARPRO',\n",
        "                'Data e Hora da Captura': 'DATHOR'\n",
        "            }, inplace=True)\n",
        "\n",
        "            # Tratamento dos campos de data e hora\n",
        "            df['DATHOR'] = pd.to_datetime(df['DATHOR'])\n",
        "            df['DATCAP'] = df['DATHOR'].dt.strftime('%d/%m/%Y')\n",
        "            df['HORCAP'] = df['DATHOR'].dt.strftime('%H:%M:%S')\n",
        "            df.drop('DATHOR', axis=1, inplace=True)\n",
        "\n",
        "            # Removendo caracteres especiais do campo QTAVAL\n",
        "            df['QTAVAL'] = df['QTAVAL'].astype(str).str.replace('[()]', '', regex=True)\n",
        "\n",
        "            # Convertendo todos os valores para texto\n",
        "            for col in df.columns.difference(['DATCAP', 'HORCAP']):\n",
        "                df[col] = df[col].astype(str)\n",
        "\n",
        "            # Adicionando os campos de data e hora de geração com fuso horário de São Paulo\n",
        "            now_sao_paulo = datetime.now(timezone_sao_paulo)\n",
        "            df['DATGER'] = now_sao_paulo.strftime('%d/%m/%Y')\n",
        "            df['HORGER'] = now_sao_paulo.strftime('%H:%M:%S')\n",
        "\n",
        "            # Filtrar duplicatas baseando-se nas chaves\n",
        "            keys = df[['URLPRO', 'DATCAP', 'HORCAP']].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
        "            df = df[~keys.isin(existing_keys.set_index(['URLPRO', 'DATCAP', 'HORCAP']).index)]\n",
        "\n",
        "            if not df.empty:\n",
        "                df.to_sql(table_name, conn, if_exists='append', index=False)\n",
        "                # Acumula os dados\n",
        "                all_data = pd.concat([all_data, df], ignore_index=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao processar o arquivo {filename}: {e}\")\n",
        "\n",
        "    # Fecha a conexão com o banco de dados\n",
        "    conn.close()\n",
        "\n",
        "    # Ordena os dados combinados\n",
        "    all_data.sort_values(by=['DATCAP', 'HORCAP'], ascending=[False, False], inplace=True)\n",
        "\n",
        "    # Salva os dados acumulados em um arquivo Excel\n",
        "    all_data.to_excel(output_excel_path, index=False)\n",
        "    print(f\"Todos os dados foram salvos em {output_excel_path}\")\n",
        "\n",
        "# Caminho para a pasta onde estão os arquivos, banco de dados e o arquivo Excel de saída\n",
        "data_folder = '/content/drive/Othercomputers/Casa/hipometrics/scraping/_mercadoLivre/2.details'\n",
        "db_file = '/content/drive/Othercomputers/Casa/hipometrics/database/bronze/BRZ001.db'\n",
        "table_name = 'BRZ001'\n",
        "output_excel_path = '/content/drive/Othercomputers/Casa/hipometrics/database/bronze/BRZ001_combined.xlsx'\n",
        "\n",
        "# Se você quiser processar uma data específica, defina-a aqui no formato 'AAAAMMDD'.\n",
        "# Se não, deixe process_date como None para usar a data atual.\n",
        "process_date = None  # ou '20240414' para uma data específica\n",
        "\n",
        "# Executar o script\n",
        "conn = create_db_connection(db_file)\n",
        "if conn is not None:\n",
        "    create_table(conn, table_name)\n",
        "    process_files_to_db(data_folder, db_file, table_name, output_excel_path, process_date)\n",
        "    conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "190t5-SAp0oI",
        "outputId": "4776107c-d56f-4f37-fb55-b614dd754539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conexão estabelecida com SQLite\n",
            "Conexão estabelecida com SQLite\n"
          ]
        }
      ]
    }
  ]
}